{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5726430d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.10Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -otobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution - (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\KIIT\\\\anaconda3\\\\Lib\\\\site-packages\\\\tensorflow\\\\python\\\\_pywrap_dtensor_device.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached tensorflow-2.10.0-cp39-cp39-win_amd64.whl (455.9 MB)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorflow==2.10) (24.3.25)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorflow==2.10) (3.11.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorflow==2.10) (1.64.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorflow==2.10) (0.31.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorflow==2.10) (1.6.3)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorflow==2.10) (18.1.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorflow==2.10) (2.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorflow==2.10) (1.14.1)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorflow==2.10) (2.10.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorflow==2.10) (21.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorflow==2.10) (63.4.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorflow==2.10) (3.19.6)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorflow==2.10) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorflow==2.10) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorflow==2.10) (1.24.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorflow==2.10) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorflow==2.10) (3.3.0)\n",
      "Collecting tensorboard<2.11,>=2.10\n",
      "  Using cached tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorflow==2.10) (0.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorflow==2.10) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorflow==2.10) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorflow==2.10) (4.11.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow==2.10) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (2.30.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (2.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (2.32.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from packaging->tensorflow==2.10) (3.0.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (5.3.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (2024.6.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (1.26.11)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10) (3.2.2)\n",
      "Installing collected packages: tensorboard, tensorflow\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.16.2\n",
      "    Uninstalling tensorboard-2.16.2:\n",
      "      Successfully uninstalled tensorboard-2.16.2\n"
     ]
    }
   ],
   "source": [
    "pip install -U tensorflow==2.10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41ef5645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers==0.13.3\n",
      "  Using cached tokenizers-0.13.3-cp39-cp39-win_amd64.whl (3.5 MB)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "Successfully installed tokenizers-0.13.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -otobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution - (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\kiit\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install tokenizers==0.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddfe1292",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "import torch\n",
    "from datasets import load_metric\n",
    "\n",
    "# Load the training and development JSON datasets\n",
    "with open(\"C:\\\\Users\\\\KIIT\\\\Desktop\\\\CSV files\\\\EXIST 2024 Tweets Dataset\\\\training\\\\EXIST2024_training.json\", 'r', encoding='utf-8') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "with open(\"C:\\\\Users\\\\KIIT\\\\Desktop\\\\CSV files\\\\EXIST 2024 Tweets Dataset\\\\dev\\\\EXIST2024_dev.json\", 'r', encoding='utf-8') as file:\n",
    "    dev_data = json.load(file)\n",
    "\n",
    "# Convert the JSON data to DataFrames\n",
    "def json_to_df(data):\n",
    "    records = []\n",
    "    for key, value in data.items():\n",
    "        record = {\n",
    "            'id': value['id_EXIST'],\n",
    "            'lang': value['lang'],\n",
    "            'tweet': value['tweet'],\n",
    "            'labels_task1': value['labels_task1'],\n",
    "            'split': value['split']\n",
    "        }\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "train_df = json_to_df(train_data)\n",
    "val_df = json_to_df(dev_data)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/LaBSE')\n",
    "\n",
    "# Encode the data\n",
    "def encode_data(tokenizer, df, max_length=128):\n",
    "    texts = df['tweet'].tolist()\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "train_encodings = encode_data(tokenizer, train_df)\n",
    "val_encodings = encode_data(tokenizer, val_df)\n",
    "\n",
    "# Encode labels for task1\n",
    "train_labels = [1 if label == 'YES' else 0 for label in train_df['labels_task1'].apply(lambda x: x[0]).tolist()]\n",
    "val_labels = [1 if label == 'YES' else 0 for label in val_df['labels_task1'].apply(lambda x: x[0]).tolist()]\n",
    "\n",
    "# Define the Dataset class\n",
    "class HinglishDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = HinglishDataset(train_encodings, train_labels)\n",
    "val_dataset = HinglishDataset(val_encodings, val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35db487b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/f1/f1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169ba3d382504c0d9f5f4fb6d206ea85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/precision/precision.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90cd5902c6bb45ef9861257359149650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/recall/recall.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb7dab5ac484cf39ec03c79781b5e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/LaBSE and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1299' max='1299' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1299/1299 5:29:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.527800</td>\n",
       "      <td>0.604762</td>\n",
       "      <td>0.708092</td>\n",
       "      <td>0.708099</td>\n",
       "      <td>0.708801</td>\n",
       "      <td>0.708092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.583700</td>\n",
       "      <td>0.565024</td>\n",
       "      <td>0.721580</td>\n",
       "      <td>0.720306</td>\n",
       "      <td>0.723752</td>\n",
       "      <td>0.721580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.204500</td>\n",
       "      <td>0.714775</td>\n",
       "      <td>0.725434</td>\n",
       "      <td>0.723613</td>\n",
       "      <td>0.729099</td>\n",
       "      <td>0.725434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='131' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 09:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7254335260115607\n",
      "Validation F1: 0.7236131688113638\n",
      "Validation Precision: 0.7290988563320879\n",
      "Validation Recall: 0.7254335260115607\n"
     ]
    }
   ],
   "source": [
    "# Load the accuracy metric\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "f1_metric = load_metric(\"f1\")\n",
    "precision_metric = load_metric(\"precision\")\n",
    "recall_metric = load_metric(\"recall\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    return {\n",
    "        'accuracy': accuracy['accuracy'],\n",
    "        'f1': f1['f1'],\n",
    "        'precision': precision['precision'],\n",
    "        'recall': recall['recall']\n",
    "    }\n",
    "\n",
    "# Initialize the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained('sentence-transformers/LaBSE', num_labels=2)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_task1',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_task1',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on the validation set\n",
    "val_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "print(f\"Validation Accuracy: {val_results['eval_accuracy']}\")\n",
    "print(f\"Validation F1: {val_results['eval_f1']}\")\n",
    "print(f\"Validation Precision: {val_results['eval_precision']}\")\n",
    "print(f\"Validation Recall: {val_results['eval_recall']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "190f462b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7186897880539499\n",
      "Validation F1: 0.7180142252401315\n",
      "Validation Precision: 0.7235599747197653\n",
      "Validation Recall: 0.7186897880539499\n",
      "Test Accuracy: 0.7321772639691715\n",
      "Test F1: 0.7290827408509127\n",
      "Test Precision: 0.7345156104090736\n",
      "Test Recall: 0.7321772639691715\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the development set into validation and test sets\n",
    "val_df, test_df = train_test_split(val_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Encode the data\n",
    "val_encodings = encode_data(tokenizer, val_df)\n",
    "test_encodings = encode_data(tokenizer, test_df)\n",
    "\n",
    "# Encode labels for task1\n",
    "val_labels = [1 if label == 'YES' else 0 for label in val_df['labels_task1'].apply(lambda x: x[0]).tolist()]\n",
    "test_labels = [1 if label == 'YES' else 0 for label in test_df['labels_task1'].apply(lambda x: x[0]).tolist()]\n",
    "\n",
    "# Create dataset objects\n",
    "val_dataset = HinglishDataset(val_encodings, val_labels)\n",
    "test_dataset = HinglishDataset(test_encodings, test_labels)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "val_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "print(f\"Validation Accuracy: {val_results['eval_accuracy']}\")\n",
    "print(f\"Validation F1: {val_results['eval_f1']}\")\n",
    "print(f\"Validation Precision: {val_results['eval_precision']}\")\n",
    "print(f\"Validation Recall: {val_results['eval_recall']}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(f\"Test Accuracy: {test_results['eval_accuracy']}\")\n",
    "print(f\"Test F1: {test_results['eval_f1']}\")\n",
    "print(f\"Test Precision: {test_results['eval_precision']}\")\n",
    "print(f\"Test Recall: {test_results['eval_recall']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3eae925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/f1/f1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/precision/precision.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/recall/recall.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/LaBSE and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1085' max='1085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1085/1085 7:51:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.604900</td>\n",
       "      <td>0.628645</td>\n",
       "      <td>0.647399</td>\n",
       "      <td>0.636220</td>\n",
       "      <td>0.676338</td>\n",
       "      <td>0.647399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.536000</td>\n",
       "      <td>0.612808</td>\n",
       "      <td>0.705202</td>\n",
       "      <td>0.697080</td>\n",
       "      <td>0.738193</td>\n",
       "      <td>0.705202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>0.568901</td>\n",
       "      <td>0.745665</td>\n",
       "      <td>0.745665</td>\n",
       "      <td>0.745665</td>\n",
       "      <td>0.745665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.429100</td>\n",
       "      <td>0.632642</td>\n",
       "      <td>0.720617</td>\n",
       "      <td>0.719364</td>\n",
       "      <td>0.728153</td>\n",
       "      <td>0.720617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.279600</td>\n",
       "      <td>0.702654</td>\n",
       "      <td>0.728324</td>\n",
       "      <td>0.727501</td>\n",
       "      <td>0.734160</td>\n",
       "      <td>0.728324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 04:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7456647398843931\n",
      "Validation F1: 0.7456647398843931\n",
      "Validation Precision: 0.7456647398843931\n",
      "Validation Recall: 0.7456647398843931\n",
      "Test Accuracy: 0.7360308285163777\n",
      "Test F1: 0.7362963109376626\n",
      "Test Precision: 0.7370042791590253\n",
      "Test Recall: 0.7360308285163777\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "import torch\n",
    "from datasets import load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Load the training and development JSON datasets\n",
    "with open(\"C:\\\\Users\\\\KIIT\\\\Desktop\\\\CSV files\\\\EXIST 2024 Tweets Dataset\\\\training\\\\EXIST2024_training.json\", 'r', encoding='utf-8') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "with open(\"C:\\\\Users\\\\KIIT\\\\Desktop\\\\CSV files\\\\EXIST 2024 Tweets Dataset\\\\dev\\\\EXIST2024_dev.json\", 'r', encoding='utf-8') as file:\n",
    "    dev_data = json.load(file)\n",
    "\n",
    "# Convert the JSON data to DataFrames\n",
    "def json_to_df(data):\n",
    "    records = []\n",
    "    for key, value in data.items():\n",
    "        record = {\n",
    "            'id': value['id_EXIST'],\n",
    "            'lang': value['lang'],\n",
    "            'tweet': value['tweet'],\n",
    "            'labels_task1': value['labels_task1'],\n",
    "            'split': value['split']\n",
    "        }\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "train_df = json_to_df(train_data)\n",
    "val_df = json_to_df(dev_data)\n",
    "\n",
    "# Split the development set into validation and test sets\n",
    "val_df, test_df = train_test_split(val_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/LaBSE')\n",
    "\n",
    "# Encode the data\n",
    "def encode_data(tokenizer, df, max_length=128):\n",
    "    texts = df['tweet'].tolist()\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "train_encodings = encode_data(tokenizer, train_df)\n",
    "val_encodings = encode_data(tokenizer, val_df)\n",
    "test_encodings = encode_data(tokenizer, test_df)\n",
    "\n",
    "# Encode labels for task1\n",
    "train_labels = [1 if label == 'YES' else 0 for label in train_df['labels_task1'].apply(lambda x: x[0]).tolist()]\n",
    "val_labels = [1 if label == 'YES' else 0 for label in val_df['labels_task1'].apply(lambda x: x[0]).tolist()]\n",
    "test_labels = [1 if label == 'YES' else 0 for label in test_df['labels_task1'].apply(lambda x: x[0]).tolist()]\n",
    "\n",
    "# Define the Dataset class\n",
    "class HinglishDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = HinglishDataset(train_encodings, train_labels)\n",
    "val_dataset = HinglishDataset(val_encodings, val_labels)\n",
    "test_dataset = HinglishDataset(test_encodings, test_labels)\n",
    "\n",
    "# Load the metrics\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "f1_metric = load_metric(\"f1\")\n",
    "precision_metric = load_metric(\"precision\")\n",
    "recall_metric = load_metric(\"recall\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    return {\n",
    "        'accuracy': accuracy['accuracy'],\n",
    "        'f1': f1['f1'],\n",
    "        'precision': precision['precision'],\n",
    "        'recall': recall['recall']\n",
    "    }\n",
    "\n",
    "# Initialize the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained('sentence-transformers/LaBSE', num_labels=2)\n",
    "\n",
    "# Updated Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_task1',\n",
    "    num_train_epochs=5,  # Increased number of epochs\n",
    "    per_device_train_batch_size=32,  # Increased batch size if hardware allows\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=1000,  # Increased warmup steps\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_task1',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,  # Save only the last 2 checkpoints\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    learning_rate=2e-5,  # Adjusted learning rate\n",
    "    metric_for_best_model=\"accuracy\",  # Monitor accuracy to save the best model\n",
    "    greater_is_better=True  # Accuracy is better when higher\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Trainer with Early Stopping Callback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Early stopping if no improvement for 3 epochs\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on the validation set\n",
    "val_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "print(f\"Validation Accuracy: {val_results['eval_accuracy']}\")\n",
    "print(f\"Validation F1: {val_results['eval_f1']}\")\n",
    "print(f\"Validation Precision: {val_results['eval_precision']}\")\n",
    "print(f\"Validation Recall: {val_results['eval_recall']}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(f\"Test Accuracy: {test_results['eval_accuracy']}\")\n",
    "print(f\"Test F1: {test_results['eval_f1']}\")\n",
    "print(f\"Test Precision: {test_results['eval_precision']}\")\n",
    "print(f\"Test Recall: {test_results['eval_recall']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4726d10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_10852\\712307651.py:74: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  accuracy_metric = load_metric(\"accuracy\")\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/f1/f1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/precision/precision.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/recall/recall.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/LaBSE and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1085' max='2170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1085/2170 14:38:25 < 14:40:03, 0.02 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.550300</td>\n",
       "      <td>0.602557</td>\n",
       "      <td>0.703614</td>\n",
       "      <td>0.698321</td>\n",
       "      <td>0.719980</td>\n",
       "      <td>0.703614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.525200</td>\n",
       "      <td>0.573539</td>\n",
       "      <td>0.733735</td>\n",
       "      <td>0.733147</td>\n",
       "      <td>0.736165</td>\n",
       "      <td>0.733735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.427300</td>\n",
       "      <td>0.597331</td>\n",
       "      <td>0.731325</td>\n",
       "      <td>0.730902</td>\n",
       "      <td>0.733091</td>\n",
       "      <td>0.731325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.226800</td>\n",
       "      <td>0.836207</td>\n",
       "      <td>0.681928</td>\n",
       "      <td>0.675373</td>\n",
       "      <td>0.698959</td>\n",
       "      <td>0.681928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.123300</td>\n",
       "      <td>1.160745</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.695052</td>\n",
       "      <td>0.709867</td>\n",
       "      <td>0.698795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 04:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7337349397590361\n",
      "Validation F1: 0.7331474402915423\n",
      "Validation Precision: 0.7361645381789601\n",
      "Validation Recall: 0.7337349397590361\n",
      "Test Accuracy: 0.7115384615384616\n",
      "Test F1: 0.7126152123352572\n",
      "Test Precision: 0.7157913515056372\n",
      "Test Recall: 0.7115384615384616\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "import torch\n",
    "from datasets import load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Load the training and development JSON datasets\n",
    "with open(\"C:\\\\Users\\\\KIIT\\\\Desktop\\\\CSV files\\\\EXIST 2024 Tweets Dataset\\\\training\\\\EXIST2024_training.json\", 'r', encoding='utf-8') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "with open(\"C:\\\\Users\\\\KIIT\\\\Desktop\\\\CSV files\\\\EXIST 2024 Tweets Dataset\\\\dev\\\\EXIST2024_dev.json\", 'r', encoding='utf-8') as file:\n",
    "    dev_data = json.load(file)\n",
    "\n",
    "# Convert the JSON data to DataFrames\n",
    "def json_to_df(data):\n",
    "    records = []\n",
    "    for key, value in data.items():\n",
    "        record = {\n",
    "            'id': value['id_EXIST'],\n",
    "            'lang': value['lang'],\n",
    "            'tweet': value['tweet'],\n",
    "            'labels_task1': value['labels_task1'],\n",
    "            'split': value['split']\n",
    "        }\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "train_df = json_to_df(train_data)\n",
    "val_df = json_to_df(dev_data)\n",
    "\n",
    "# Split the development set into validation and test sets\n",
    "val_df, test_df = train_test_split(val_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/LaBSE')\n",
    "\n",
    "# Encode the data\n",
    "def encode_data(tokenizer, df, max_length=128):\n",
    "    texts = df['tweet'].tolist()\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "train_encodings = encode_data(tokenizer, train_df)\n",
    "val_encodings = encode_data(tokenizer, val_df)\n",
    "test_encodings = encode_data(tokenizer, test_df)\n",
    "\n",
    "# Encode labels for task1\n",
    "train_labels = [1 if label == 'YES' else 0 for label in train_df['labels_task1'].apply(lambda x: x[0]).tolist()]\n",
    "val_labels = [1 if label == 'YES' else 0 for label in val_df['labels_task1'].apply(lambda x: x[0]).tolist()]\n",
    "test_labels = [1 if label == 'YES' else 0 for label in test_df['labels_task1'].apply(lambda x: x[0]).tolist()]\n",
    "\n",
    "# Define the Dataset class\n",
    "class HinglishDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = HinglishDataset(train_encodings, train_labels)\n",
    "val_dataset = HinglishDataset(val_encodings, val_labels)\n",
    "test_dataset = HinglishDataset(test_encodings, test_labels)\n",
    "\n",
    "# Load the metrics\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "f1_metric = load_metric(\"f1\")\n",
    "precision_metric = load_metric(\"precision\")\n",
    "recall_metric = load_metric(\"recall\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    return {\n",
    "        'accuracy': accuracy['accuracy'],\n",
    "        'f1': f1['f1'],\n",
    "        'precision': precision['precision'],\n",
    "        'recall': recall['recall']\n",
    "    }\n",
    "\n",
    "# Initialize the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained('sentence-transformers/LaBSE', num_labels=2)\n",
    "\n",
    "# Updated Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_task1',\n",
    "    num_train_epochs=10,  # Further increased number of epochs\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,  # Adjusted warmup steps\n",
    "    weight_decay=0.005,  # Tuned weight decay parameter\n",
    "    logging_dir='./logs_task1',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=3e-5,  # Increased learning rate slightly\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Trainer with Early Stopping Callback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on the validation set\n",
    "val_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "print(f\"Validation Accuracy: {val_results['eval_accuracy']}\")\n",
    "print(f\"Validation F1: {val_results['eval_f1']}\")\n",
    "print(f\"Validation Precision: {val_results['eval_precision']}\")\n",
    "print(f\"Validation Recall: {val_results['eval_recall']}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(f\"Test Accuracy: {test_results['eval_accuracy']}\")\n",
    "print(f\"Test F1: {test_results['eval_f1']}\")\n",
    "print(f\"Test Precision: {test_results['eval_precision']}\")\n",
    "print(f\"Test Recall: {test_results['eval_recall']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5884853b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
